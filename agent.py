#!/usr/bin/env python
# -*- coding: utf-8 -*-

import asyncio, os, sys, json
from contextlib import AsyncExitStack
from dotenv import load_dotenv
from mcp import ClientSession, StdioServerParameters
from mcp.client.stdio import stdio_client
from langchain_mcp_adapters.tools import load_mcp_tools
from langgraph.prebuilt import create_react_agent
from langchain_google_genai import ChatGoogleGenerativeAI
from langgraph.checkpoint.memory import MemorySaver

load_dotenv()

class CustomEncoder(json.JSONEncoder):
    def default(self, o):
        if hasattr(o, "content"):
            return {"type": o.__class__.__name__, "content": o.content}
        return super().default(o)

def read_config_json():
    config_path = os.getenv("CONFIG")
    if not config_path:
        script_dir = os.path.dirname(os.path.abspath(__file__))
        config_path = os.path.join(script_dir, "config.json")
        print(f"âš ï¸  CONFIG not set. Falling back to: {config_path}")
    try:
        with open(config_path, "r") as f:
            return json.load(f)
    except Exception as e:
        print(f"âŒ Failed to read config file at '{config_path}': {e}")
        sys.exit(1)

# â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”
# 1) LLM
# â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”
llm = ChatGoogleGenerativeAI(
    model="gemini-2.0-flash",
    temperature=0,
    max_retries=3,
    google_api_key=os.getenv("GOOGLE_API_KEY")
)

# â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”
# 2) å»ºç«‹ Agent
# â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”
async def build_agent():
    cfg = read_config_json()
    mcp_servers = cfg.get("mcpServers", {})
    if not mcp_servers:
        print("âŒ No MCP servers in config"); sys.exit(1)

    tools = []
    stack = AsyncExitStack()
    await stack.__aenter__()
    for name, info in mcp_servers.items():
        print(f"ğŸ”— Connecting to MCP Server: {name}")
        params = StdioServerParameters(command=info["command"], args=info["args"], env=info["env"])
        read, write = await stack.enter_async_context(stdio_client(params))
        session = await stack.enter_async_context(ClientSession(read, write))
        await session.initialize()
        server_tools = await load_mcp_tools(session)
        tools.extend(server_tools)
        print(f"âœ… Loaded {len(server_tools)} tools from {name}")

    if not tools:
        print("âŒ No tools loaded"); sys.exit(1)

    # we only care about scale_deployment here
    system_prompt = (
    "ä½ æ˜¯ MySQL å°ˆå®¶åŠ©æ‰‹ï¼Œæœƒå¹«åŠ©ä½¿ç”¨è€…å°è³‡æ–™åº«é€²è¡ŒæŸ¥è©¢ã€è§€å¯Ÿã€èˆ‡çµæ§‹æ¢ç´¢ã€‚\n"
    "è«‹æ ¹æ“šä½¿ç”¨è€…è¼¸å…¥ï¼Œè‡ªå‹•ä½¿ç”¨ `execute_query` å·¥å…·å°æ‡‰é©ç•¶çš„ SQL æŸ¥è©¢ã€‚\n"
    "ä¾‹å¦‚ï¼š\n"
    "- è‹¥ä½¿ç”¨è€…è¼¸å…¥ 'show db'ï¼Œè«‹å‘¼å« execute_query ä¸¦åŸ·è¡Œ SQL: SHOW DATABASES;\n"
    "- è‹¥è¼¸å…¥ 'æŸ¥çœ‹æŸè³‡æ–™è¡¨çµæ§‹'ï¼Œè«‹è½‰æ›æˆ 'DESCRIBE table_name' çš„ SQL èªå¥ã€‚\n"
    "æ°¸é ä½¿ç”¨å·¥å…·å®Œæˆä»»å‹™ï¼Œä¸è¦è‡ªå·±å›ç­”ã€‚")



    memory = MemorySaver()
    agent = create_react_agent(
        llm,
        tools=tools,
        prompt=system_prompt,
        checkpointer=memory
    )
    return agent, system_prompt, stack 

# â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”
# 3) CLI + Multi-turn
# â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”
async def main():
    agent, system_prompt, stack = await build_agent()
    thread_id = "auto-0001"
    config = {"configurable": {"thread_id": thread_id}}

    # åˆå§‹ messages åŒ…å« system prompt è§’è‰²
    messages = [{"role":"system", "content": system_prompt}]

    print("\nğŸš€ DBNormalizeBot ready! Enter 'latency replicas' or 'quit'.")
    while True:
        line = input("Query (latency replicas): ").strip()
        if not line or line.lower() == "quit":
            break

        # append user message
        messages.append({"role":"user", "content": line})

        # Agent.ainvoke æœƒè‡ªå‹•å¤šè¼ªèª¿ç”¨ç›´åˆ°èƒŒå¾Œ LLM å› FINAL ANSWER
        resp = await agent.ainvoke({"messages": messages}, config)

        print("\nğŸ“¨ Response:")
        print(json.dumps(resp, indent=2, ensure_ascii=False, cls=CustomEncoder))

        # è‹¥æ˜¯ FINAL ANSWERï¼ŒçµæŸæœ¬æ¬¡ loop
        if isinstance(resp, str) and resp.startswith("FINAL ANSWER"):
            # é‡ç½® messagesï¼ˆæˆ– break çœ‹éœ€æ±‚ï¼‰
            messages = [{"role":"system", "content": system_prompt}]
            print("ğŸ”„ reset context for next run\n")


if __name__ == "__main__":
    asyncio.run(main())
